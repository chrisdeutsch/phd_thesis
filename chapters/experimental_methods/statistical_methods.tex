% ==============================================================================
\section{Statistical Inference}%
\label{sec:statistical_inference}
% ==============================================================================

This section introduces techniques of statistical inference commonly used in the
ATLAS experiment to interpret the results of searches and measurements. Among
these are methods for parameter estimation, interval estimation, and statistical
hypothesis testing. These are used to fit statistical models to observed data
and to evaluate whether the data indicate the presence of a signal. In this
thesis, statistical software based on \texttt{HistFactory}~\cite{cranmer2012},
\texttt{RooFit}~\cite{Verkerke:2003ir}, and
\texttt{RooStats}~\cite{Moneta:2010pm} is employed for the interpretation of the
results. The following discussion is restricted to statistical models of count
data in a set of disjoint bins. The notation of Ref.~\cite{cranmer2012} is
adopted with few modifications.

% Particle physics workhorse of statistical interpretation:

% - Method of maximum likelihood for point estimation

% - Likelihood ratio tests (or closely related to the Likelihood ratio test) for
% hypothesis testing, estimation of confidence intervals (or limit)

% - Restrict to performing statistical interpretations using binned distributions.

\subsection{The Statistical Model}

In high-energy physics, the use of binned data is widespread for visualisation
and statistical modelling. Analyses of collider data typically consider multiple
mutually disjoint regions, also referred to as \emph{channels}, defined by
certain event selections. Each region consists of one or more bins defined by a
discriminating variable.

\todo[inline]{Maybe say something about signals / backgrounds and define what a
  'physics process' is? Often count events in bins...}

Let $\mathcal{C}$ denote the set of channels and $\mathcal{B}_{c}$ the set of
bins in channel~$c$. The probability of observing $n_{cb}$ events in bin~$b$ of
channel~$c$ is modelled by a Poisson distribution with probability mass function
$\pois(n_{cb} \vert \nu_{cb})$, where $\nu_{cb}$ denotes the (unknown) expected
number of events for the given bin. The expectation $\nu_{cb}$, which has to be
inferred from the observed data, is parameterised as
\begin{align*}
  \nu_{cb}(\myvec{\alpha}, \myvec{\phi}, \myvec{\gamma}) =
  \sum_{s \in \mathcal{S}} \, \gamma_{csb} \, \phi_{cs} \, \eta_{cs}(\myvec{\alpha}) \, \sigma_{csb}(\myvec{\alpha}) \,\text{,}
\end{align*}
where $\myvec{\alpha}$, $\myvec{\phi}$, $\myvec{\gamma}$ are parameters, and
$\mathcal{S}$ is the set of physics processes that are included in the
model~\cite{cranmer2012}. The parameters $\myvec{\alpha}$ and $\myvec{\gamma}$
are nuisance parameters (NPs) with external constraints, which will be
introduced shortly. The relevance of the four factors $\gamma_{csb}$,
$\phi_{cs}$, $\eta_{cs}$, and $\sigma_{csb}$ is described in the
following~\cite{cranmer2012}:
\begin{itemize}

\item $\sigma_{csb}(\myvec{\alpha})$ is referred to as the \emph{parameterised
    histogram} of process~$s$ in channel~$c$, the index $b$ denoting the bins of
  the histogram. It serves to estimate the expected number of events from
  process~$s$ in channel~$c$ and is usually derived using simulation or control
  region data. The parameterised histogram has additional degrees of freedom,
  parameterised by $\myvec{\alpha}$, to account for uncertainties on the shape
  of the histogram. These degrees of freedom leave the overall normalisation
  given by $\sum_{b \in \mathcal{B}_{c}} \sigma_{csb}(\myvec{\alpha})$
  unchanged.

\item $\eta_{cs}(\myvec{\alpha})$ represents a normalisation factor applied
  uniformly to all bins of the parameterised histogram for a given process~$s$
  in channel~$c$. The normalisation factor $\eta_{cs}$ is not allowed to vary
  freely since it is a function of the NPs~$\myvec{\alpha}$, the NPs being
  subject to external constraints. This factor is included to account for
  uncertainties on the normalisation of process~$s$ in channel~$c$.

\item $\phi_{cs}$\todo{This is not correct: should be a product of normalisation
    factors} represents a \emph{free} normalisation factor applied uniformly to
  all bins of the parameterised histogram for a given process~$s$ in
  channel~$c$. These parameters are optional, i.e.\ they can be fixed to unity
  for a given process/channel depending on the statistical model one wants to
  construct.
  % Normalisation factors that are shared?
  In most cases, at least one normalisation factor is present that is applied to
  the physics process of interest, also referred to as the \emph{signal}. This
  normalisation factor is referred to as the \emph{signal strength}, $\mu$, and
  is often considered to be the parameter of interest (POI). Normalisation
  factors are considered to be NPs if they are not POIs.

\item $\gamma_{csb}$ are parameters that introduce additional degrees of freedom
  for every channel, process, and bin. They are used to incorporate
  uncertainties that originate from sources that are independent between
  channels, processes, and bins. An example of such an uncertainty is the
  statistical uncertainty arising from the use of finite samples of events to
  estimate $\sigma_{csb}$.

  In this thesis, the method by Barlow and Beeston~\cite{barlow1993} is used to
  account for the statistical uncertainties on $\sigma_{csb}$. To reduce the
  number of parameters of the statistical model, the method is simplified as
  proposed in Ref.~\cite{conway2011} by only considering the statistical
  uncertainty on $\sum_{s \in \mathcal{S}} \sigma_{csb}$ in a given
  bin/channel. Consequently, the parameters $\gamma_{csb}$ can be replaced by
  $\gamma_{cb}$, omitting the dependence on the physics process.

\end{itemize}
A description of the exact functional form of $\sigma_{csb}(\myvec{\alpha})$ and
$\eta_{cs}(\myvec{\alpha})$ is omitted here but can be found in
Ref.~\cite{cranmer2012}. The likelihood function of the statistical model is
given by
\begin{align}
  L(\myvec{\alpha}, \myvec{\phi}, \myvec{\gamma}) = \Biggl[\,
  \prod_{c \in \mathcal{C}}
  \prod_{b \in \mathcal{B}_{c}}
  \pois\bigl( n_{cb} \big| \nu_{cb}(\myvec{\alpha}, \myvec{\phi}, \myvec{\gamma}) \bigr)
  \,\Biggr]
  \times L_{\text{ext}}(\myvec{\alpha}, \myvec{\gamma}) \,\text{,}
  \label{eq:likelihood_histfactory}
\end{align}
where $L_{\text{ext}}(\myvec{\alpha}, \myvec{\gamma})$ is the likelihood
defining the external constraints on the NPs~$\myvec{\alpha}$ and
$\myvec{\gamma}$. The external constraints are defined by
\begin{align*}
  L_{\text{ext}}(\myvec{\alpha}, \myvec{\gamma}) =
  \Biggl[\, \prod_{p} f(a_p \vert \alpha_p)     \,\Biggr]
  \Biggl[\, \prod_{c \in \mathcal{C}} \prod_{b \in \mathcal{B}_{c}} \pois(m_{cb} \vert \gamma_{cb} \tau_{cb}) \,\Biggr] \,\text{,}
\end{align*}
with the two terms in brackets
\begin{itemize}

\item The first term describes the constraints

  $a = 0$ such that the MLE of $\hat{\alpha} = 0$

  $f(a \vert \alpha)$ denotes the probability density of a Normal distribution
  with unit variance and mean of $\alpha$.

  In isolation, this term describes an auxiliary measurement yielding a
  $\alpha_p$ confidence interval of $[-1, 1]$ at \SI{68}{\percent} confidence
  level. Therefore, the



  \SI{68}{\percent} confidence interval is given by $\alpha_p \in [-1, 1]$,
  hence $\alpha_p = \pm 1$ is interpreted as $\pm 1 \sigma$



\item

\end{itemize}


where the first bracket defines the constraints on
$\myvec{\alpha} = (\alpha_p)_p$ and the second one



The statistical model of
the auxiliary measurements constraining the NPs $\myvec{\alpha}$ is constructed
from







The statistical model of the auxiliary measurements constraining the NPs
$\myvec{\alpha}$ is closely connected to the functional form of
$\sigma_{csb}(\myvec{\alpha})$ and $\eta_{cs}(\myvec{\alpha})$, thus only a
general introduction is given.





These measurements are usually defined by performing comparisons of


with alternative predictions that characterise systematic uncertainties





Often, these
measurements are defined by a comparison





The likelihood function of the auxiliary measurement is given by


In this thesis, $f_p(a_p \vert \alpha_p)$ denotes the probability density of a
Normal distribution with mean~$\alpha_p$ and unit variance.


Maximum likelihood estimation is used to estimate model parameters from fits to
data.

Asimov dataset.


\subsection{Hypothesis Testing}

- In the following let the likelihood function be denoted
$L(\mu, \myvec{\theta})$ to separate the POI, $\mu$, from the nuisance
parameters $\myvec{\theta}$.

- Generally want to test statements of particular values of the signal strength
$\mu$

- The statistical model is nested

- Tests are based on variations of the likelihood ratio test

\begin{align*}
  \Lambda(\mu) = -2 \ln \left[ \frac{ L\bigl(\mu, \hat{\hat{\myvec{\theta}}}(\mu) \bigr)}{L(\muhat, \hat{\myvec{\theta}})} \right]
\end{align*}

Wilk's theorem.

Under the null -> $\chi^2$ distribution with $n$ degrees of freedom, where $n$
is the...

In the following we focus on a single POI, such that $n = 1$.


\subsubsection{Discovery of a Signal}

For the discovery of a signal one can readily test the nullhypothesis of
$\mu = 0$ against the alternative of $\mu \neq 1$. This can be readily performed
using $\Lambda(0)$ as the test statistic. The sampling distribution of
$\Lambda(0)$ under the null hypothesis is known in asymptotic
approximation. However, when searching for new phenomena one is generally
interested in signals with positive strength, i.e.\ $\mu > 0$. This leads to an
altered likelihood ratio test statistic given by
\begin{align*}
  q_0 = \begin{cases}
          \Lambda(0), & \muhat > 0 \\
          0,          & \muhat \leq 0
        \end{cases}
\end{align*}


The asymptotic sampling distribution of $q_0$ under the null hypothesis is given
by
\begin{align*}
  f(q_0; n) = \frac{1}{2} \delta(q_0) + \frac{1}{2} f_{\chi^2}(q_0; n) \,\text{,}
\end{align*}
which is a equal mixture of a Dirac $\delta$ distribution and a $\chi^2$
distribution with $n$ degrees of freedom, $f_{\chi^2}$.

For a given critical value of $q_0^\text{c}$ of the test, the discovery
significance (type I error rate) is given by
\begin{align*}
  p_0 = \int_{q_0^\text{crit}}^\infty \mathrm{d}q f(q; n) \overset{n = 1}{=} \sqrt{q_0^\text{crit}}
\end{align*}

The significance $p_0$ is often expressed as
\begin{align*}
  Z_0 = \Phi^{-1}(1 - p_0) \,\text{,}
\end{align*}
where $\Phi^{-1}$ is the quantile function of the Standard Normal
distribution. In particle physics, for the discovery of a signal two
significance thresholds are


two signifiance thresholds are namely $Z_0 = 3$ ($p_0 = \num{1.35e-3}$) and
$Z_0 = 5$ ($p_0 = \num{2.87e-7}$), which are the thresholds for \emph{evidence}
and \emph{discovery}, respectively.


\subsubsection{Upper Limits on the Signal Strength}



\subsection{The Method of Maximum Likelihood}

Let~$f(\myvec{x} \mid \boldtheta)$ be the joint probability
density\footnote{The principles outlined in the following also hold
  for discrete probabiltiy distributions where the equivalent of the
  probability density is the probability mass.} of a vector of random
variables~$\myvec{X} = (X_1, \dots, X_n)$ that depends on
parameters~$\boldtheta$. For a given
realisation~$\myvec{x} = (x_1, \dots, x_n)$ of~$\myvec{X}$ one can
consider the joint density at the point~$\myvec{x}$ as a function of
the parameter vector~$\boldtheta$
\begin{align*}
  L(\boldtheta \mid \boldx) = f(\boldx \mid \boldtheta) \overset{\text{i.i.d.}}{=} \prod_{i=1}^n f(x_i \mid \boldtheta)
\end{align*}
which is called the likelihood function. The last equality holds for
independent and identically
distributed~$X_i \sim f(x \mid \boldtheta)$. The likelihood function
connects the probability density of the (fixed)
observation~$\myvec{x}$ with the parameter vector~\boldtheta. The
parameters of the joint distribution can be inferred from the
observation using of maximum likelihood estimation by
\begin{align*}
  \hat{\boldtheta}(\boldx) = \argmax_{\boldtheta} L(\boldtheta \mid \boldx)
\end{align*}
where~$\hat{\boldtheta}$ is an estimator of the parameters based on
the observed value of~\myvec{X}, i.e.\ the estimator is chosen such
that the likelihood of observing~\boldx is maximised.

The likelihood function is a fundamental part of frequentist
statistical inference used to analyse results of high-energy physics
experiments. Maximum likelihood estimators have convenient properties
in the large sample limit under some general regularity
conditions\footnote{For a detailed description of these conditions see
  for example~\cite{casella2001}.} on the likelihood
function. Let~$\hat{\theta}_n$ denote the maximum likelihood estimator
for~$\theta$ on a sample of size~$n$, then the estimator has the
following asymptotic properties~\cite{casella2001}:
\begin{description}
\item[Consistency] The estimator converges to the true value~$\theta$
  with an arbitrarily small deviation as the sample size tends to
  infinity, i.e.\
  $\lim_{n \ra \infty} P(|\hat{\theta}_n - \theta| < \epsilon) = 1$
  for any $\epsilon > 0$. In this limit the estimator is unbiased with
  vanishing variance.

\item[Asymptotic normality and efficiency] In the large sample
  limit~$\sqrt{n}(\hat{\theta}_n - \theta)$ converges in distribution
  to a normal distribution with zero mean and variance~$v(\theta)$,
  where~$\theta$ is the true value of the parameter and~$v(\theta)$
  the \textit{asymptotic variance} of the estimator. The estimator is
  \textit{asymptotically efficient} in the sense that~$v(\theta)$
  reaches the Cram\'er--Rao lower bound for the asymptotic variance of
  estimators.
\end{description}

Frequently one minimises the negative logarithm of the likelihood
(NLL) instead, which being a monotonic transformation does not change
the parameter values extremum, to convert the products / exponentials
frequently appearing in the likelihood function into sums / factors.

The probability density function~$f$ is frequently intractable
analytically but can be estimated using Monte Carlo
simulation. Therefore, one can discretise the the observable of
interest into~$k$~bins such that the probability for an event to fall
into bin~$i$ can be described by
\begin{align*}
  p_i(\boldtheta) = \int_\text{bin} f(x; \boldtheta) \, \mathrm{d}x
\end{align*}
or using frequentist methods when only MC simulation is available. In
the case of the binned distribution, the likelihood function for a
fixed sample of size~$n$ is given by the probability mass function of
the multinomial distribution
\begin{align*}
  L(\boldtheta \mid \boldn) = \frac{n!}{\prod_{i=1}^k n_i!} \prod_{i = 1}^k \left[ p_i(\boldtheta) \right]^{n_i}
\end{align*}
where~$n_i$ corresponds to the number of observations in bin~$i$ with
the condition that
\begin{align*}
  \sum_{i = 1}^k n_i = n \quad \text{and} \quad \sum_{i = 1}^k p_i(\boldtheta) = 1 \,\text{.}
\end{align*}

In high energy physics, the size of the collected dataset is not fixed
a priori but is varying according to a Poisson distribution with an
expected rate~$\nu$. This is accounted for in the so-called ``extended
likelihood function''~\cite{cowan1998}:
\begin{align*}
  L(\nu, \boldtheta \mid \boldn) =
  \frac{\nu^n e^{-\nu}}{n!} \times \frac{n!}
  {\prod_{i = 1}^k n_{i}!}
  \prod_{i = 1}^k \left[ p_i(\boldtheta) \right]^{n_i}
\end{align*}
which follows from the joint distribution given by the product of the
Poisson and multinomial probability mass functions (conditional on the
number of events observed?).  With the expectation value of the number
of events in bin~$i$ given
by~$\nu_i(\boldtheta) = \nu p_i(\boldtheta)$\todo{Can $\nu$ depend on
  \boldtheta? In principle it can and should!?} and the previously
mentioned conditions it follows that
\begin{align*}
  L(\nu_1, \dots, \nu_k, \boldtheta \mid \boldn) = \prod_{i = 1}^k \frac{\nu_i^{n_i} e^{-\nu_i}}{n_i!}
\end{align*}
which is the product of Poisson probabilities of observing~$n_i$
events in a bin with an expected number of events~$\nu_i$ (the
dependency on \boldtheta is omitted).

% The Poisson part gives the probability of observing~$n$ events while
% the multinomial describes the probability of distributing these events
% among~$k$ bins. In general~$\nu_i$ can depend on~\boldtheta.

In general we are interested in mixture models, where events can
originate from different processes and the observable, which is called
the discriminant in this context as it aims to distinguish between the
different parts of the mixture, follows a process-specific
distribution. For illustration consider two species of events, one
from a signal process with PDF $f_\text{S}(x; \boldtheta)$ and from a
background process with PDF $f_\text{B}(x; \boldtheta)$. Consider a
model where the signal process contributes with a fraction~$\alpha$
such that the PDF of the mixture is:
\begin{align*}
  f(x; \mu, \boldtheta) = \alpha f_\text{S}(x; \boldtheta) + (1 - \alpha) f_\text{B}(x; \boldtheta) \,\text{.}
\end{align*}
In practice~$f_\text{B}$ is frequently itself a mixture model
consisting of multiple background processes. ($f_\text{S}$ should only
depend on $\boldtheta_\text{s}$ same for bkg.?) In the binned representation this
mixture model yields:
\begin{align*}
  \nu_i(\mu, \boldtheta) = \mu \nu_i^\text{S}(\boldtheta) + \nu_i^\text{B}(\boldtheta)
\end{align*}
with the signal strength~$\mu$ being a parameter of the model, which
is frequently expressed as the cross section of the signal process
with respect to a baseline value like the SM expectation.

The parameter~$\mu$ of the mixture model is usually the parameter of
interest (POI) while the other parameters~\boldtheta, which need to be
determined to estimate the signal strength, are called nuisance
parameters.

\todo[inline]{Maybe stress that at this point it's a particular choice
  model?}

The binned / extended likelihood function is further extended by
incorporating multiple analysis channels, additional free parameters
(like the signal strength), systematic uncertainties, and auxiliary
measurements. With a set of analysis channels~$\mathcal{C}$, a set of
bins in a given channel~$\mathcal{B}_\mathcal{C}$, and a set of
auxiliary measurements~$\mathbb{S}$, the likelihood function can be
written as~\cite{cranmer2012}:
\begin{align*}
  L(\boldalpha, \boldphi, \boldgamma) =
  \prod_{c \in \mathcal{C}} \prod_{b \in \mathcal{B}_{\mathcal{C}}}
  \mathrm{Pois}\left( n_{cb} \mid \nu_{cb}(\boldalpha, \boldphi, \boldgamma) \right)
  \mathrm{Pois}\left(m_{cb} \mid  \gamma_{cb} \tau_{cb} \right)
  \prod_{p \in \mathbb{S}} f_p\left(a_p \mid \alpha_p \right)
\end{align*}
Latin letter correspond to observables while greek letters correspond
to nuisance parameters.

\begin{align*}
  \nu_{cb}(\boldalpha, \boldphi, \boldgamma) = \sum_{s \in \mathcal{S}} \gamma_{cb} \, \phi_{cs} \, \eta_{cs}(\boldalpha) \, \sigma_{csb}(\boldalpha)
\end{align*}
$\boldalpha = (\alpha_i)_i$, $\boldphi = (\phi_{cs})_{cs}$,
$\boldgamma = (\gamma_{cb})$.

\todo[inline]{Describe: histogram-based uncertainties, normalisation
  uncertainties, free normalisation factors, MC stat uncertainty}

\todo[inline]{Describe what is meant by 'observables' and 'global
  observables'.}


\subsection{Treatment of Statistical Uncertainties on Background Rate
  Predictions}%
\label{sec:barlow_beeston}

The predicted background rate in a given bin $b$ of a channel $c$ is
determined from a finite sample of events, e.g.\ from MC simulation,
and thus does not directly correspond to the true expected rate. The
background rate estimates are subject to uncertainties that have to be
considered when performing inference, particularly when bins are only
sparsely populated with events.

This uncertainty is included in the likelihood function, employing the
method proposed by Barlow and Beeston~\cite{barlow1993}, by replacing
the expected background rate from the prediction using the finite
sample with the true expected rate that has to be simultaneously
inferred. In practice this is done by performing the substitution
$\nu_{cb} \rightarrow \gamma_{cb} \nu_{cb}$ that introduces new
nuisance parameters $\gamma_{cb}$ specifying the relative difference
between the predicted and true expected rates. Initially it was
proposed to introduce one such nuisance parameter for every background
source~\cite{barlow1993}, however, a simplified version proposed in
Ref.~\cite{conway2011} is used such that only the combined uncertainty
on the total background prediction is considered instead.

The $\gamma$ nuisance parameters are constrained by auxiliary
measurements using the observed samples of events entering the
bins. These measurements contribute terms of the form
\begin{align*}
  \pois(m_{cb} | \gamma_{cb} \tau_{cb})
  \qquad \text{with} \qquad
  \tau_{cb} = \frac{( \sum_i w_i )^2}{\sum_i w_i^2} = \text{const.}
\end{align*}
to the likelihood function~\cite{cranmer2012}, where the sums go over
all events contributing to bin $b$ in channel $c$ with event weights
$w_i$. This corresponds to a measurement of the effective number of
events based on the observed value $m_{cb}$, which is nominally equal
to $\tau_{cb}$\footnote{It should be noted that $m_{cb}$ are generally
  not integer-valued quantities and thus do not agree with the support
  of the Poisson distribution. Thus the factorial term in the Poisson
  PMF is replaced by the continuous gamma function to generalise the
  distribution to $\mathbb{R}^+$.}, from the finite sample.

This approach is based on the approximation of the \emph{compound
  Poisson distribution} (CPD) that describes the distribution of the
sum of a Poisson number of random weights with a \emph{scaled Poisson
  distribution} (SPD)~\cite{Bohm:2013gla}. The CPD describes the
distribution of rate predictions based on a finite sample of weighted
events and can be defined as
\begin{align*}
  X = \sum_{i = 1}^{N} W_i \quad \text{with} \quad N \sim \pois(\lambda) \quad \text{and} \quad \text{i.i.d.\ } W_i \text{ (independent of }N\text{)} \,\text{.}
\end{align*}
It can be approximated, see for example Ref.~\cite{Bohm:2013gla},
using a scaled Poisson distribution defined by
\begin{align*}
  \tilde{X} = s \cdot \tilde{N} \quad \text{with} \quad \tilde{N} \sim \pois(\tilde{\lambda})
\end{align*}
with
\begin{align*}
  s = \frac{\expect(W^2)}{\expect(W)} \qquad \tilde{\lambda} = \frac{\lambda \expect(W)^2}{\expect(W^2)}
\end{align*}
where $\expect(W)$ and $\expect(W^2)$ are the first and second moment
of the weight distribution. The Barlow-Beeston method makes the
assumption that the expectation values can be approximated by sample
averages such that
\begin{align*}
  s = \frac{\sum_i w_i^2}{\sum_i w_i} \qquad \tilde{\lambda} = \frac{\lambda}{n} \frac{(\sum_i w_i)^2}{\sum_i w_i^2}
\end{align*}
with sample size $n$. Comparing the equation for $\tilde{\lambda}$
with the constraint term entering the likelihood function illustrates
the connection to a measurement of the effective number of events. A
number of $\tilde{N}$ events contribute with equal weights given by
$s$ to the sum, thus $\tilde{N}$ can be referred to as an effective
number of events.

These relationships will be needed in the context of the statistical
interpretation of the results in~\Cref{sec:toys_global_observables}.


\subsection{Hypothesis Testing}
\label{sec:hypothesis_testing}

What is a hypothesis? A population that is described by a specific set
of parameters. Hypothesis tests distinguish between two complementary
hypotheses. Hypothesis test provides a rule to either accept (reject)
the null hypothesis, rejecting (accepting) the alternative based on an
observed sample from a population.


Hypothesis testing aims to distinguish between a null hypothesis~$H_0$
(e.g.\ the Standard Model) and an alternative hypothesis~$H_1$. The
value of a test statistic~$t(\mathbf{x})$, which is commonly a real
number, serves to indicate agreement with $H_0$ or $H_1$,
respectively. Various methods exist to construct suitable test
statistics.

A rejection region of the test can be defined by setting a threshold
on the test statistic, i.e.\ $t > t_\text{cut}$, which specifies the
significance~$\alpha$ of the test, which is the probability to reject
$H_0$ in favour of $H_1$ if $H_0$ was true (type I error
rate). Moreover, the power of the test, $1 - \beta$, is specified,
which is related to the probability~$\beta$ of accepting $H_1$ when
$H_0$ was true (type II error rate). The field of particle physics
adopts the convention that the null hypothesis needs to be rejected at
a significance level of~$\alpha = \num{2.87e-7}$ ($5\sigma$) to claim
discovery of new phenomena (which is intentionally low to prevent
false discoveries). Exclusion of signal models is usually done at 95\%
confidence level ($\alpha = 0.05$).


Composite hypothesis: hypothesis depending on some parameter, e.g.\
signal strength, nuisance parameters.


$\boldtheta \in \boldsymbol{\Theta}_0$ (usually a simple hypothesis,
i.e.\ the set has one element)

$\boldtheta \in \boldsymbol{\Theta}_1$ (composite hypothesis)

Nested model: The b-only model is a limiting case of the
alternative~$\mu \ra 0$.



\subsubsection{Likelihood Ratio Test and Neyman-Pearson Lemma}

The (profile) Likelihood Ratio Test is the optimal test based on the
Neyman-Pearson lemma~\cite{neyman1933} meaning for a given
significance level yielding the largest power.
\begin{align*}
  \lambda = \frac{L(\boldtheta \mid H_0)}{L(\boldtheta \mid H_1)}
\end{align*}

\todo[inline]{The Likelihood Ratio Test is only the optimal test if
  there are no free parameters.}

Likelihood ratio test for nested / composite models (i.e.\ the
alternativ model contains the null hypothesis as a
subset~$\Theta_0 \subset \Theta$:
\begin{align}
  \lambda = \frac{\sup_{\theta \in \Theta_0} L(\theta \mid \boldx)}{\sup_{\theta \in \Theta\phantom{_0}} L(\theta \mid \boldx)}
\end{align}
where the numerator is the restricted MLE over $\Theta_0$ and the
numerator the unrestricted MLE. Frequently, the test statistics are
expressed as~$-2 \ln \lambda$ due to the asymptotic properties of the
sampling distributions.

The likelihood ratio still depends on both the parameters of interest
as well as the nuisance parameters. The profile likelihood given by
\begin{align*}
  L_\text{p}(\mu) = \sup_\theta L(\mu, \theta) % \\
%  \hat{\hat{\theta}}(\mu) = \argmax_\theta \mathcal{L}(\mu, \theta \mid \mathbf{x})
\end{align*}
and represents a profile of the likelihood function along the line
given by~$\hat{\hat{\theta}}(\mu) = \argmax_\theta L(\mu, \theta)$
(sometimes called the conditional maximum likelihood estimate of the
nuisance parameters given~$\mu$ -- double hat to distinguish it from
the maximum likelihood estimate~$\hat{\theta}$). The maximum of the
profile likelihood coincides with the maximum likelihood estimate.


In HEP we usually have a model where the signal contribution is
controlled by the signal strenght~$\mu \geq 0$ where~$\mu = 0$
corresponds to the background-only hypothesis. Thus the likelihood
ratio test for a null hypothesis with signal strength~$\mu$ is written as
\begin{align}
  \lambda(\mu) = \frac{\sup_\theta L(\mu, \theta \mid \boldx)}{\sup_{\mu,\theta} L(\mu, \theta \mid \boldx)}
\end{align}


\begin{align*}
  \Lambda(\mu) = \frac{\mathcal{L}\left( \mu, \hat{\hat{\theta}} \mid \mathbf{x} \right)}
                      {\mathcal{L}\left( \hat{\mu}, \hat{\theta} \mid \mathbf{x} \right)}
\end{align*}


hat: estimator to distinguish it from the true value of the parameter.

Asymptotic distributions: Wilk's theorem (valid under some regularity
conditions: asymptotic, nested, either model is correct, identifiable,
i.e.\ any value of parameters is uniquely assigned to a distinct
hypothesis)



\subsubsection{Discovery Significance}


The test statistic for the discovery of a positive signal
is~\cite{Cowan:2010js}:
\begin{align}
  q_0 =
  \begin{cases}
    -2 \log \lambda(0) & \muhat \geq 0 \\
    0 & \muhat < 0
  \end{cases}
\end{align}




Asymptotic distribution: non-central chi-square with degrees of
freedom according to the number of POIs. In the case of the null
hypothesis, this reduces to the result from Wilk (non-centrality
factor is zero) but really it is a half chi-square distribution.


\subsubsection{Confidence Intervals \& Upper Limits}

A confidence interval (comparison to point estimate, e.g.\ MLE) is an
interval of a parameter derived from the observed sample that covers
the true value with a given coverage probabilty. (The interval is the
random quantity and not the parameter).

The calculation of confidence intervals and upper limits can be
depicted as an inversion of hypothesis tests. The question is which
$\mu$ null hypothesis would be rejected at a fixed significance
level~$\alpha$. The confidence interval is then the region which
cannot be rejected at the specified coverage $1 - \alpha$.

Asymptotic properties of likelihood ratio -> should be roughly normal
in the minimum, can be taylor expanded and then confidence intervals
can be extracted from the Hessian at the MLE.


\begin{align}
  \tilde{q}_\mu =
  \begin{cases}
    -2 \ln \tilde{\lambda}(\mu) & \muhat \leq \mu \\
    0 & \muhat > \mu
  \end{cases}
\end{align}

Asimov (for CLs method)?

CLs method: \cite{Read:2002hq}

\begin{align}
  \text{CL}_\text{s+b} = \int^\infty_{q_\text{obs}} f(\tilde{q}_\mu \mid \mu) \, \mathrm{d}\tilde{q}_\mu \\
  \text{CL}_\text{b} = \int^\infty_{q_\text{obs}} f(\tilde{q}_\mu \mid 0) \, \mathrm{d}\tilde{q}_\mu
\end{align}


\begin{align}
  \text{CL}_\text{s} = \frac{\text{CL}_\text{s+b}}{\text{CL}_\text{b}}
\end{align}



% Statistical Analysis:

% Hypothesis testing: Neyman Pearson lemma, Likelihood ratio (\LambdaÎ›) test
% Wilk's theorem (asymptotic approximation)
% Discovery significance (q0)
% Limit setting (CLs / CLs+b)

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../phd_thesis"
%%% End:
